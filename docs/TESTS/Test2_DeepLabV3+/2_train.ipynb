{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a434e4ac-725c-4307-8714-c2fa3d086402",
   "metadata": {},
   "source": [
    "## DeepLabV3 Semantic segmentation on Cityscapes dataset\n",
    "Based on:\n",
    "https://github.com/fregu856/deeplabv3/tree/master\n",
    "\n",
    "Others examples:\n",
    "- https://github.com/CoinCheung/DeepLab-v3-plus-cityscapes\n",
    "- https://github.com/VainF/DeepLabV3Plus-Pytorch?tab=readme-ov-file\n",
    "- https://github.com/chenxi116/DeepLabv3.pytorch\n",
    "- https://github.com/giovanniguidi/deeplabV3-PyTorch/tree/master\n",
    "\n",
    "### Environment Pytorch39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513239b9-a065-44e7-b94d-97092bfccde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "current_path=os.getcwd()\n",
    "\n",
    "# imported from datasets.py\n",
    "from datasets import DatasetTrain, DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n",
    "\n",
    "# imported from /model/deeplabv3.py\n",
    "sys.path.append(current_path+\"/model\")\n",
    "from deeplabv3 import DeepLabV3\n",
    "\n",
    "# imported from /utils/utils.py\n",
    "sys.path.append(current_path+\"/utils\")\n",
    "from utils import add_weight_decay\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74154ff5-28f0-4a3e-8f17-e2266ab73d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained resnet, 18\n",
      "num_train_batches: 991\n",
      "num_val_batches: 166\n"
     ]
    }
   ],
   "source": [
    "# NOTE! NOTE! change this to not overwrite all log data when you train the model:\n",
    "model_id = \"1\"\n",
    "\n",
    "num_epochs = 300\n",
    "batch_size = 3\n",
    "learning_rate = 0.0001\n",
    "\n",
    "network = DeepLabV3(model_id, project_dir=current_path).cuda()\n",
    "\n",
    "train_dataset = DatasetTrain(cityscapes_data_path=current_path+\"/data/cityscapes\",\n",
    "                             cityscapes_meta_path=current_path+\"/data/cityscapes/meta\")\n",
    "val_dataset = DatasetVal(cityscapes_data_path=current_path+\"/data/cityscapes\",\n",
    "                         cityscapes_meta_path=current_path+\"/data/cityscapes/meta\")\n",
    "\n",
    "num_train_batches = int(len(train_dataset)/batch_size)\n",
    "num_val_batches = int(len(val_dataset)/batch_size)\n",
    "print (\"num_train_batches:\", num_train_batches)\n",
    "print (\"num_val_batches:\", num_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc2059a-d29d-4d3c-a4a7-2746d2af6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size, shuffle=False,\n",
    "                                         num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40611c9d-bf7a-4d46-b000-0154027bef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = add_weight_decay(network, l2_value=0.0001)\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "with open(current_path+\"/data/cityscapes/meta/class_weights.pkl\", \"rb\") as file: # (needed for python3)    \n",
    "    class_weights = np.array(pickle.load(file))\n",
    "class_weights = torch.from_numpy(class_weights)\n",
    "class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972f4d0-3ddd-467a-b574-f940416b6f3e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08dcf5cb-d4f4-4270-b142-bc5c803e2eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "epoch: 1/300\n",
      "223.29293704032898\n",
      "train loss: 1.84293\n",
      "####\n",
      "###########################\n",
      "epoch: 2/300\n",
      "221.00732707977295\n",
      "train loss: 1.43343\n",
      "####\n",
      "###########################\n",
      "epoch: 3/300\n",
      "218.3480622768402\n",
      "train loss: 1.31758\n",
      "####\n",
      "###########################\n",
      "epoch: 4/300\n",
      "221.02988719940186\n",
      "train loss: 1.24397\n",
      "####\n",
      "###########################\n",
      "epoch: 5/300\n",
      "220.00473594665527\n",
      "train loss: 1.19078\n",
      "####\n",
      "###########################\n",
      "epoch: 6/300\n",
      "218.4487748146057\n",
      "train loss: 1.10872\n",
      "####\n",
      "###########################\n",
      "epoch: 7/300\n",
      "218.63233542442322\n",
      "train loss: 1.10276\n",
      "####\n",
      "###########################\n",
      "epoch: 8/300\n",
      "219.36339259147644\n",
      "train loss: 1.03531\n",
      "####\n",
      "###########################\n",
      "epoch: 9/300\n",
      "219.26627254486084\n",
      "train loss: 1.02967\n",
      "####\n",
      "###########################\n",
      "epoch: 10/300\n",
      "219.81028771400452\n",
      "train loss: 1.03096\n",
      "####\n",
      "###########################\n",
      "epoch: 11/300\n",
      "217.8560974597931\n",
      "train loss: 0.974849\n",
      "####\n",
      "###########################\n",
      "epoch: 12/300\n",
      "218.96096324920654\n",
      "train loss: 0.944385\n",
      "####\n",
      "###########################\n",
      "epoch: 13/300\n",
      "216.80377197265625\n",
      "train loss: 0.942857\n",
      "####\n",
      "###########################\n",
      "epoch: 14/300\n",
      "218.32493615150452\n",
      "train loss: 0.910893\n",
      "####\n",
      "###########################\n",
      "epoch: 15/300\n",
      "217.29058933258057\n",
      "train loss: 0.923429\n",
      "####\n",
      "###########################\n",
      "epoch: 16/300\n",
      "222.02074575424194\n",
      "train loss: 0.931464\n",
      "####\n",
      "###########################\n",
      "epoch: 17/300\n",
      "216.9217493534088\n",
      "train loss: 0.908176\n",
      "####\n",
      "###########################\n",
      "epoch: 18/300\n",
      "220.33123254776\n",
      "train loss: 0.903762\n",
      "####\n",
      "###########################\n",
      "epoch: 19/300\n",
      "217.3557369709015\n",
      "train loss: 0.886289\n",
      "####\n",
      "###########################\n",
      "epoch: 20/300\n",
      "220.0801019668579\n",
      "train loss: 0.870704\n",
      "####\n",
      "###########################\n",
      "epoch: 21/300\n",
      "217.95470356941223\n",
      "train loss: 0.873363\n",
      "####\n",
      "###########################\n",
      "epoch: 22/300\n",
      "220.2126760482788\n",
      "train loss: 0.859269\n",
      "####\n",
      "###########################\n",
      "epoch: 23/300\n",
      "220.39978861808777\n",
      "train loss: 0.868319\n",
      "####\n",
      "###########################\n",
      "epoch: 24/300\n",
      "216.16426753997803\n",
      "train loss: 0.858898\n",
      "####\n",
      "###########################\n",
      "epoch: 25/300\n",
      "217.64559292793274\n",
      "train loss: 0.836622\n",
      "####\n",
      "###########################\n",
      "epoch: 26/300\n",
      "245.21072053909302\n",
      "train loss: 0.853632\n",
      "####\n",
      "###########################\n",
      "epoch: 27/300\n",
      "251.31416988372803\n",
      "train loss: 0.82142\n",
      "####\n",
      "###########################\n",
      "epoch: 28/300\n",
      "217.83792066574097\n",
      "train loss: 0.83669\n",
      "####\n",
      "###########################\n",
      "epoch: 29/300\n",
      "220.73093724250793\n",
      "train loss: 0.823705\n",
      "####\n",
      "###########################\n",
      "epoch: 30/300\n",
      "218.77606797218323\n",
      "train loss: 0.814371\n",
      "####\n",
      "###########################\n",
      "epoch: 31/300\n",
      "218.00455951690674\n",
      "train loss: 0.820528\n",
      "####\n",
      "###########################\n",
      "epoch: 32/300\n",
      "218.31515550613403\n",
      "train loss: 0.811078\n",
      "####\n",
      "###########################\n",
      "epoch: 33/300\n",
      "217.82934379577637\n",
      "train loss: 0.794113\n",
      "####\n",
      "###########################\n",
      "epoch: 34/300\n",
      "216.59616899490356\n",
      "train loss: 0.803572\n",
      "####\n",
      "###########################\n",
      "epoch: 35/300\n",
      "218.19923329353333\n",
      "train loss: 0.76641\n",
      "####\n",
      "###########################\n",
      "epoch: 36/300\n",
      "218.87206745147705\n",
      "train loss: 0.810596\n",
      "####\n",
      "###########################\n",
      "epoch: 37/300\n",
      "216.90665483474731\n",
      "train loss: 0.793143\n",
      "####\n",
      "###########################\n",
      "epoch: 38/300\n",
      "217.3155927658081\n",
      "train loss: 0.775863\n",
      "####\n",
      "###########################\n",
      "epoch: 39/300\n",
      "218.12754154205322\n",
      "train loss: 0.782006\n",
      "####\n",
      "###########################\n",
      "epoch: 40/300\n",
      "218.0490584373474\n",
      "train loss: 0.812583\n",
      "####\n",
      "###########################\n",
      "epoch: 41/300\n",
      "220.27200436592102\n",
      "train loss: 0.774907\n",
      "####\n",
      "###########################\n",
      "epoch: 42/300\n",
      "239.92644715309143\n",
      "train loss: 0.76191\n",
      "####\n",
      "###########################\n",
      "epoch: 43/300\n",
      "226.45506954193115\n",
      "train loss: 0.774541\n",
      "####\n",
      "###########################\n",
      "epoch: 44/300\n",
      "227.68606638908386\n",
      "train loss: 0.766461\n",
      "####\n",
      "###########################\n",
      "epoch: 45/300\n",
      "227.35389852523804\n",
      "train loss: 0.780645\n",
      "####\n",
      "###########################\n",
      "epoch: 46/300\n",
      "226.2150101661682\n",
      "train loss: 0.766646\n",
      "####\n",
      "###########################\n",
      "epoch: 47/300\n",
      "230.89320349693298\n",
      "train loss: 0.736808\n",
      "####\n",
      "###########################\n",
      "epoch: 48/300\n",
      "231.6913559436798\n",
      "train loss: 0.762058\n",
      "####\n",
      "###########################\n",
      "epoch: 49/300\n",
      "230.16536045074463\n",
      "train loss: 0.728936\n",
      "####\n",
      "###########################\n",
      "epoch: 50/300\n",
      "263.671532869339\n",
      "train loss: 0.766981\n",
      "####\n",
      "###########################\n",
      "epoch: 51/300\n",
      "254.35313391685486\n",
      "train loss: 0.748214\n",
      "####\n",
      "###########################\n",
      "epoch: 52/300\n",
      "235.51818752288818\n",
      "train loss: 0.752273\n",
      "####\n",
      "###########################\n",
      "epoch: 53/300\n",
      "228.9945182800293\n",
      "train loss: 0.729297\n",
      "####\n",
      "###########################\n",
      "epoch: 54/300\n",
      "217.36760306358337\n",
      "train loss: 0.769093\n",
      "####\n",
      "###########################\n",
      "epoch: 55/300\n",
      "219.74914741516113\n",
      "train loss: 0.730316\n",
      "####\n",
      "###########################\n",
      "epoch: 56/300\n",
      "249.36029696464539\n",
      "train loss: 0.744887\n",
      "####\n",
      "###########################\n",
      "epoch: 57/300\n",
      "217.74250864982605\n",
      "train loss: 0.72466\n",
      "####\n",
      "###########################\n",
      "epoch: 58/300\n",
      "217.71789240837097\n",
      "train loss: 0.73592\n",
      "####\n",
      "###########################\n",
      "epoch: 59/300\n",
      "217.55567383766174\n",
      "train loss: 0.71948\n",
      "####\n",
      "###########################\n",
      "epoch: 60/300\n",
      "218.55939984321594\n",
      "train loss: 0.744244\n",
      "####\n",
      "###########################\n",
      "epoch: 61/300\n",
      "217.18838691711426\n",
      "train loss: 0.770299\n",
      "####\n",
      "###########################\n",
      "epoch: 62/300\n",
      "219.39826130867004\n",
      "train loss: 0.735461\n",
      "####\n",
      "###########################\n",
      "epoch: 63/300\n",
      "229.11874318122864\n",
      "train loss: 0.730257\n",
      "####\n",
      "###########################\n",
      "epoch: 64/300\n",
      "235.46529603004456\n",
      "train loss: 0.730173\n",
      "####\n",
      "###########################\n",
      "epoch: 65/300\n",
      "231.7827489376068\n",
      "train loss: 0.729976\n",
      "####\n",
      "###########################\n",
      "epoch: 66/300\n",
      "229.93502044677734\n",
      "train loss: 0.72314\n",
      "####\n",
      "###########################\n",
      "epoch: 67/300\n",
      "301.35479259490967\n",
      "train loss: 0.710218\n",
      "####\n",
      "###########################\n",
      "epoch: 68/300\n",
      "297.14074778556824\n",
      "train loss: 0.706189\n",
      "####\n",
      "###########################\n",
      "epoch: 69/300\n",
      "246.00343418121338\n",
      "train loss: 0.716676\n",
      "####\n",
      "###########################\n",
      "epoch: 70/300\n",
      "242.5282003879547\n",
      "train loss: 0.717625\n",
      "####\n",
      "###########################\n",
      "epoch: 71/300\n",
      "241.51036143302917\n",
      "train loss: 0.727248\n",
      "####\n",
      "###########################\n",
      "epoch: 72/300\n",
      "219.0087194442749\n",
      "train loss: 0.708166\n",
      "####\n",
      "###########################\n",
      "epoch: 73/300\n",
      "222.52942824363708\n",
      "train loss: 0.700852\n",
      "####\n",
      "###########################\n",
      "epoch: 74/300\n",
      "232.85280990600586\n",
      "train loss: 0.715029\n",
      "####\n",
      "###########################\n",
      "epoch: 75/300\n",
      "253.6138837337494\n",
      "train loss: 0.728934\n",
      "####\n",
      "###########################\n",
      "epoch: 76/300\n",
      "232.9342439174652\n",
      "train loss: 0.716532\n",
      "####\n",
      "###########################\n",
      "epoch: 77/300\n",
      "277.00308084487915\n",
      "train loss: 0.680526\n",
      "####\n",
      "###########################\n",
      "epoch: 78/300\n",
      "297.0426490306854\n",
      "train loss: 0.692279\n",
      "####\n",
      "###########################\n",
      "epoch: 79/300\n",
      "249.6089732646942\n",
      "train loss: 0.700692\n",
      "####\n",
      "###########################\n",
      "epoch: 80/300\n",
      "237.9781448841095\n",
      "train loss: 0.706071\n",
      "####\n",
      "###########################\n",
      "epoch: 81/300\n",
      "221.5942726135254\n",
      "train loss: 0.707907\n",
      "####\n",
      "###########################\n",
      "epoch: 82/300\n",
      "219.9220747947693\n",
      "train loss: 0.676512\n",
      "####\n",
      "###########################\n",
      "epoch: 83/300\n",
      "219.86555767059326\n",
      "train loss: 0.685149\n",
      "####\n",
      "###########################\n",
      "epoch: 84/300\n",
      "219.08913707733154\n",
      "train loss: 0.661461\n",
      "####\n",
      "###########################\n",
      "epoch: 85/300\n",
      "219.80298733711243\n",
      "train loss: 0.718988\n",
      "####\n",
      "###########################\n",
      "epoch: 86/300\n",
      "223.80616736412048\n",
      "train loss: 0.709876\n",
      "####\n",
      "###########################\n",
      "epoch: 87/300\n",
      "225.52314496040344\n",
      "train loss: 0.686425\n",
      "####\n",
      "###########################\n",
      "epoch: 88/300\n",
      "245.58394074440002\n",
      "train loss: 0.668187\n",
      "####\n",
      "###########################\n",
      "epoch: 89/300\n",
      "258.4292368888855\n",
      "train loss: 0.690049\n",
      "####\n",
      "###########################\n",
      "epoch: 90/300\n",
      "221.0799527168274\n",
      "train loss: 0.689087\n",
      "####\n",
      "###########################\n",
      "epoch: 91/300\n",
      "219.170756816864\n",
      "train loss: 0.712371\n",
      "####\n",
      "###########################\n",
      "epoch: 92/300\n",
      "222.74393558502197\n",
      "train loss: 0.661884\n",
      "####\n",
      "###########################\n",
      "epoch: 93/300\n",
      "281.33813881874084\n",
      "train loss: 0.676973\n",
      "####\n",
      "###########################\n",
      "epoch: 94/300\n",
      "248.81461930274963\n",
      "train loss: 0.706937\n",
      "####\n",
      "###########################\n",
      "epoch: 95/300\n",
      "241.7880527973175\n",
      "train loss: 0.678094\n",
      "####\n",
      "###########################\n",
      "epoch: 96/300\n",
      "218.84662246704102\n",
      "train loss: 0.68816\n",
      "####\n",
      "###########################\n",
      "epoch: 97/300\n",
      "219.3226683139801\n",
      "train loss: 0.680707\n",
      "####\n",
      "###########################\n",
      "epoch: 98/300\n",
      "217.7637267112732\n",
      "train loss: 0.668442\n",
      "####\n",
      "###########################\n",
      "epoch: 99/300\n",
      "217.97760009765625\n",
      "train loss: 0.666138\n",
      "####\n",
      "###########################\n",
      "epoch: 100/300\n",
      "218.17766666412354\n",
      "train loss: 0.682254\n",
      "####\n",
      "###########################\n",
      "epoch: 101/300\n",
      "218.6866376399994\n",
      "train loss: 0.686085\n",
      "####\n",
      "###########################\n",
      "epoch: 102/300\n",
      "217.3987627029419\n",
      "train loss: 0.6498\n",
      "####\n",
      "###########################\n",
      "epoch: 103/300\n",
      "217.8509397506714\n",
      "train loss: 0.685047\n",
      "####\n",
      "###########################\n",
      "epoch: 104/300\n",
      "218.2293450832367\n",
      "train loss: 0.679226\n",
      "####\n",
      "###########################\n",
      "epoch: 105/300\n",
      "218.2967004776001\n",
      "train loss: 0.669302\n",
      "####\n",
      "###########################\n",
      "epoch: 106/300\n",
      "221.47102069854736\n",
      "train loss: 0.667855\n",
      "####\n",
      "###########################\n",
      "epoch: 107/300\n",
      "218.192645072937\n",
      "train loss: 0.679724\n",
      "####\n",
      "###########################\n",
      "epoch: 108/300\n",
      "218.8320508003235\n",
      "train loss: 0.667102\n",
      "####\n",
      "###########################\n",
      "epoch: 109/300\n",
      "223.94692015647888\n",
      "train loss: 0.670763\n",
      "####\n",
      "###########################\n",
      "epoch: 110/300\n",
      "224.16247487068176\n",
      "train loss: 0.641359\n",
      "####\n",
      "###########################\n",
      "epoch: 111/300\n",
      "221.63870549201965\n",
      "train loss: 0.664255\n",
      "####\n",
      "###########################\n",
      "epoch: 112/300\n",
      "219.96817326545715\n",
      "train loss: 0.674567\n",
      "####\n",
      "###########################\n",
      "epoch: 113/300\n",
      "217.57215285301208\n",
      "train loss: 0.672902\n",
      "####\n",
      "###########################\n",
      "epoch: 114/300\n",
      "218.31750440597534\n",
      "train loss: 0.682027\n",
      "####\n",
      "###########################\n",
      "epoch: 115/300\n",
      "217.9194552898407\n",
      "train loss: 0.666428\n",
      "####\n",
      "###########################\n",
      "epoch: 116/300\n",
      "217.7975926399231\n",
      "train loss: 0.654024\n",
      "####\n",
      "###########################\n",
      "epoch: 117/300\n",
      "218.37801504135132\n",
      "train loss: 0.669704\n",
      "####\n",
      "###########################\n",
      "epoch: 118/300\n",
      "218.07123398780823\n",
      "train loss: 0.658061\n",
      "####\n",
      "###########################\n",
      "epoch: 119/300\n",
      "217.690269947052\n",
      "train loss: 0.661187\n",
      "####\n",
      "###########################\n",
      "epoch: 120/300\n",
      "216.92298460006714\n",
      "train loss: 0.647031\n",
      "####\n",
      "###########################\n",
      "epoch: 121/300\n",
      "220.91944456100464\n",
      "train loss: 0.650662\n",
      "####\n",
      "###########################\n",
      "epoch: 122/300\n",
      "217.32084846496582\n",
      "train loss: 0.636073\n",
      "####\n",
      "###########################\n",
      "epoch: 123/300\n",
      "220.00783801078796\n",
      "train loss: 0.653855\n",
      "####\n",
      "###########################\n",
      "epoch: 124/300\n",
      "221.67524552345276\n",
      "train loss: 0.657\n",
      "####\n",
      "###########################\n",
      "epoch: 125/300\n",
      "219.6597864627838\n",
      "train loss: 0.643481\n",
      "####\n",
      "###########################\n",
      "epoch: 126/300\n",
      "219.71063947677612\n",
      "train loss: 0.649694\n",
      "####\n",
      "###########################\n",
      "epoch: 127/300\n",
      "218.1799077987671\n",
      "train loss: 0.662166\n",
      "####\n",
      "###########################\n",
      "epoch: 128/300\n",
      "217.24308323860168\n",
      "train loss: 0.66566\n",
      "####\n",
      "###########################\n",
      "epoch: 129/300\n",
      "218.3227834701538\n",
      "train loss: 0.667293\n",
      "####\n",
      "###########################\n",
      "epoch: 130/300\n",
      "218.6411907672882\n",
      "train loss: 0.671461\n",
      "####\n",
      "###########################\n",
      "epoch: 131/300\n",
      "216.91228652000427\n",
      "train loss: 0.662502\n",
      "####\n",
      "###########################\n",
      "epoch: 132/300\n",
      "218.05461359024048\n",
      "train loss: 0.660651\n",
      "####\n",
      "###########################\n",
      "epoch: 133/300\n",
      "221.41340136528015\n",
      "train loss: 0.644417\n",
      "####\n",
      "###########################\n",
      "epoch: 134/300\n",
      "218.86778569221497\n",
      "train loss: 0.617055\n",
      "####\n",
      "###########################\n",
      "epoch: 135/300\n",
      "223.31625771522522\n",
      "train loss: 0.648875\n",
      "####\n",
      "###########################\n",
      "epoch: 136/300\n",
      "224.48412561416626\n",
      "train loss: 0.638818\n",
      "####\n",
      "###########################\n",
      "epoch: 137/300\n",
      "218.50379180908203\n",
      "train loss: 0.63557\n",
      "####\n",
      "###########################\n",
      "epoch: 138/300\n",
      "220.31474995613098\n",
      "train loss: 0.637977\n",
      "####\n",
      "###########################\n",
      "epoch: 139/300\n",
      "218.18792629241943\n",
      "train loss: 0.643362\n",
      "####\n",
      "###########################\n",
      "epoch: 140/300\n",
      "217.38913202285767\n",
      "train loss: 0.643472\n",
      "####\n",
      "###########################\n",
      "epoch: 141/300\n",
      "218.10542845726013\n",
      "train loss: 0.643945\n",
      "####\n",
      "###########################\n",
      "epoch: 142/300\n",
      "221.7134096622467\n",
      "train loss: 0.622801\n",
      "####\n",
      "###########################\n",
      "epoch: 143/300\n",
      "222.0105800628662\n",
      "train loss: 0.646296\n",
      "####\n",
      "###########################\n",
      "epoch: 144/300\n",
      "219.02885508537292\n",
      "train loss: 0.632655\n",
      "####\n",
      "###########################\n",
      "epoch: 145/300\n",
      "218.62456035614014\n",
      "train loss: 0.647069\n",
      "####\n",
      "###########################\n",
      "epoch: 146/300\n",
      "218.21122431755066\n",
      "train loss: 0.645781\n",
      "####\n",
      "###########################\n",
      "epoch: 147/300\n",
      "219.05150699615479\n",
      "train loss: 0.619742\n",
      "####\n",
      "###########################\n",
      "epoch: 148/300\n",
      "218.5803346633911\n",
      "train loss: 0.629034\n",
      "####\n",
      "###########################\n",
      "epoch: 149/300\n",
      "222.86954641342163\n",
      "train loss: 0.640406\n",
      "####\n",
      "###########################\n",
      "epoch: 150/300\n",
      "218.02092909812927\n",
      "train loss: 0.617665\n",
      "####\n",
      "###########################\n",
      "epoch: 151/300\n",
      "218.9899833202362\n",
      "train loss: 0.645556\n",
      "####\n",
      "###########################\n",
      "epoch: 152/300\n",
      "220.3290557861328\n",
      "train loss: 0.598993\n",
      "####\n",
      "###########################\n",
      "epoch: 153/300\n",
      "260.3241813182831\n",
      "train loss: 0.604871\n",
      "####\n",
      "###########################\n",
      "epoch: 154/300\n",
      "245.04038166999817\n",
      "train loss: 0.651541\n",
      "####\n",
      "###########################\n",
      "epoch: 155/300\n",
      "217.69173550605774\n",
      "train loss: 0.637367\n",
      "####\n",
      "###########################\n",
      "epoch: 156/300\n",
      "217.84369611740112\n",
      "train loss: 0.614637\n",
      "####\n",
      "###########################\n",
      "epoch: 157/300\n",
      "219.54659724235535\n",
      "train loss: 0.649365\n",
      "####\n",
      "###########################\n",
      "epoch: 158/300\n",
      "221.75749945640564\n",
      "train loss: 0.629958\n",
      "####\n",
      "###########################\n",
      "epoch: 159/300\n",
      "220.35222697257996\n",
      "train loss: 0.636106\n",
      "####\n",
      "###########################\n",
      "epoch: 160/300\n",
      "223.48055958747864\n",
      "train loss: 0.634167\n",
      "####\n",
      "###########################\n",
      "epoch: 161/300\n",
      "217.5734519958496\n",
      "train loss: 0.646155\n",
      "####\n",
      "###########################\n",
      "epoch: 162/300\n",
      "216.29070329666138\n",
      "train loss: 0.653737\n",
      "####\n",
      "###########################\n",
      "epoch: 163/300\n",
      "216.9443564414978\n",
      "train loss: 0.615225\n",
      "####\n",
      "###########################\n",
      "epoch: 164/300\n",
      "216.82242679595947\n",
      "train loss: 0.618181\n",
      "####\n",
      "###########################\n",
      "epoch: 165/300\n",
      "217.56844019889832\n",
      "train loss: 0.621105\n",
      "####\n",
      "###########################\n",
      "epoch: 166/300\n",
      "217.80841255187988\n",
      "train loss: 0.626056\n",
      "####\n",
      "###########################\n",
      "epoch: 167/300\n",
      "217.3719344139099\n",
      "train loss: 0.63149\n",
      "####\n",
      "###########################\n",
      "epoch: 168/300\n",
      "217.97524762153625\n",
      "train loss: 0.617132\n",
      "####\n",
      "###########################\n",
      "epoch: 169/300\n",
      "218.62358117103577\n",
      "train loss: 0.614521\n",
      "####\n",
      "###########################\n",
      "epoch: 170/300\n",
      "250.82462239265442\n",
      "train loss: 0.623939\n",
      "####\n",
      "###########################\n",
      "epoch: 171/300\n",
      "218.4909954071045\n",
      "train loss: 0.63915\n",
      "####\n",
      "###########################\n",
      "epoch: 172/300\n",
      "216.87787127494812\n",
      "train loss: 0.617664\n",
      "####\n",
      "###########################\n",
      "epoch: 173/300\n",
      "216.55826950073242\n",
      "train loss: 0.61624\n",
      "####\n",
      "###########################\n",
      "epoch: 174/300\n",
      "218.28140926361084\n",
      "train loss: 0.623382\n",
      "####\n",
      "###########################\n",
      "epoch: 175/300\n",
      "218.30408596992493\n",
      "train loss: 0.595644\n",
      "####\n",
      "###########################\n",
      "epoch: 176/300\n",
      "218.42992162704468\n",
      "train loss: 0.636555\n",
      "####\n",
      "###########################\n",
      "epoch: 177/300\n",
      "217.4339542388916\n",
      "train loss: 0.64553\n",
      "####\n",
      "###########################\n",
      "epoch: 178/300\n",
      "218.06115221977234\n",
      "train loss: 0.620955\n",
      "####\n",
      "###########################\n",
      "epoch: 179/300\n",
      "217.8525173664093\n",
      "train loss: 0.63904\n",
      "####\n",
      "###########################\n",
      "epoch: 180/300\n",
      "218.02762818336487\n",
      "train loss: 0.616029\n",
      "####\n",
      "###########################\n",
      "epoch: 181/300\n",
      "216.269464969635\n",
      "train loss: 0.626764\n",
      "####\n",
      "###########################\n",
      "epoch: 182/300\n",
      "216.2307984828949\n",
      "train loss: 0.596875\n",
      "####\n",
      "###########################\n",
      "epoch: 183/300\n",
      "218.11458730697632\n",
      "train loss: 0.618948\n",
      "####\n",
      "###########################\n",
      "epoch: 184/300\n",
      "220.10776567459106\n",
      "train loss: 0.64579\n",
      "####\n",
      "###########################\n",
      "epoch: 185/300\n",
      "222.66426515579224\n",
      "train loss: 0.630597\n",
      "####\n",
      "###########################\n",
      "epoch: 186/300\n",
      "217.9621024131775\n",
      "train loss: 0.633904\n",
      "####\n",
      "###########################\n",
      "epoch: 187/300\n",
      "224.36081075668335\n",
      "train loss: 0.625066\n",
      "####\n",
      "###########################\n",
      "epoch: 188/300\n",
      "217.89097118377686\n",
      "train loss: 0.609819\n",
      "####\n",
      "###########################\n",
      "epoch: 189/300\n",
      "217.66602969169617\n",
      "train loss: 0.616233\n",
      "####\n",
      "###########################\n",
      "epoch: 190/300\n",
      "218.10174703598022\n",
      "train loss: 0.629081\n",
      "####\n",
      "###########################\n",
      "epoch: 191/300\n",
      "218.0217936038971\n",
      "train loss: 0.629395\n",
      "####\n",
      "###########################\n",
      "epoch: 192/300\n",
      "217.98760676383972\n",
      "train loss: 0.611124\n",
      "####\n",
      "###########################\n",
      "epoch: 193/300\n",
      "217.3724865913391\n",
      "train loss: 0.62172\n",
      "####\n",
      "###########################\n",
      "epoch: 194/300\n",
      "216.9916889667511\n",
      "train loss: 0.620125\n",
      "####\n",
      "###########################\n",
      "epoch: 195/300\n",
      "216.45097255706787\n",
      "train loss: 0.607965\n",
      "####\n",
      "###########################\n",
      "epoch: 196/300\n",
      "217.56927347183228\n",
      "train loss: 0.603645\n",
      "####\n",
      "###########################\n",
      "epoch: 197/300\n",
      "217.5461084842682\n",
      "train loss: 0.615859\n",
      "####\n",
      "###########################\n",
      "epoch: 198/300\n",
      "216.77626585960388\n",
      "train loss: 0.612433\n",
      "####\n",
      "###########################\n",
      "epoch: 199/300\n",
      "216.98948693275452\n",
      "train loss: 0.617284\n",
      "####\n",
      "###########################\n",
      "epoch: 200/300\n",
      "216.7881042957306\n",
      "train loss: 0.619221\n",
      "####\n",
      "###########################\n",
      "epoch: 201/300\n",
      "215.71368503570557\n",
      "train loss: 0.616289\n",
      "####\n",
      "###########################\n",
      "epoch: 202/300\n",
      "217.61908864974976\n",
      "train loss: 0.629118\n",
      "####\n",
      "###########################\n",
      "epoch: 203/300\n",
      "216.12740087509155\n",
      "train loss: 0.615788\n",
      "####\n",
      "###########################\n",
      "epoch: 204/300\n",
      "216.82461714744568\n",
      "train loss: 0.613707\n",
      "####\n",
      "###########################\n",
      "epoch: 205/300\n",
      "216.12858295440674\n",
      "train loss: 0.610425\n",
      "####\n",
      "###########################\n",
      "epoch: 206/300\n",
      "216.81484079360962\n",
      "train loss: 0.629731\n",
      "####\n",
      "###########################\n",
      "epoch: 207/300\n",
      "217.30750465393066\n",
      "train loss: 0.59928\n",
      "####\n",
      "###########################\n",
      "epoch: 208/300\n",
      "215.9760491847992\n",
      "train loss: 0.621473\n",
      "####\n",
      "###########################\n",
      "epoch: 209/300\n",
      "217.36773824691772\n",
      "train loss: 0.603291\n",
      "####\n",
      "###########################\n",
      "epoch: 210/300\n",
      "223.77434134483337\n",
      "train loss: 0.608744\n",
      "####\n",
      "###########################\n",
      "epoch: 211/300\n",
      "217.76587414741516\n",
      "train loss: 0.592767\n",
      "####\n",
      "###########################\n",
      "epoch: 212/300\n",
      "217.51557517051697\n",
      "train loss: 0.61147\n",
      "####\n",
      "###########################\n",
      "epoch: 213/300\n",
      "216.9605312347412\n",
      "train loss: 0.591858\n",
      "####\n",
      "###########################\n",
      "epoch: 214/300\n",
      "220.0129644870758\n",
      "train loss: 0.616486\n",
      "####\n",
      "###########################\n",
      "epoch: 215/300\n",
      "216.45073008537292\n",
      "train loss: 0.606115\n",
      "####\n",
      "###########################\n",
      "epoch: 216/300\n",
      "217.10837125778198\n",
      "train loss: 0.601482\n",
      "####\n",
      "###########################\n",
      "epoch: 217/300\n",
      "216.1086187362671\n",
      "train loss: 0.602471\n",
      "####\n",
      "###########################\n",
      "epoch: 218/300\n",
      "225.88797330856323\n",
      "train loss: 0.602755\n",
      "####\n",
      "###########################\n",
      "epoch: 219/300\n",
      "268.6474597454071\n",
      "train loss: 0.601978\n",
      "####\n",
      "###########################\n",
      "epoch: 220/300\n",
      "230.33403253555298\n",
      "train loss: 0.608942\n",
      "####\n",
      "###########################\n",
      "epoch: 221/300\n",
      "217.30311369895935\n",
      "train loss: 0.590741\n",
      "####\n",
      "###########################\n",
      "epoch: 222/300\n",
      "216.12585496902466\n",
      "train loss: 0.597813\n",
      "####\n",
      "###########################\n",
      "epoch: 223/300\n",
      "217.53802275657654\n",
      "train loss: 0.581505\n",
      "####\n",
      "###########################\n",
      "epoch: 224/300\n",
      "216.9021565914154\n",
      "train loss: 0.605913\n",
      "####\n",
      "###########################\n",
      "epoch: 225/300\n",
      "217.31799864768982\n",
      "train loss: 0.608014\n",
      "####\n",
      "###########################\n",
      "epoch: 226/300\n",
      "217.1138129234314\n",
      "train loss: 0.631136\n",
      "####\n",
      "###########################\n",
      "epoch: 227/300\n",
      "217.48277020454407\n",
      "train loss: 0.586024\n",
      "####\n",
      "###########################\n",
      "epoch: 228/300\n",
      "217.19178938865662\n",
      "train loss: 0.625478\n",
      "####\n",
      "###########################\n",
      "epoch: 229/300\n",
      "216.29812622070312\n",
      "train loss: 0.624044\n",
      "####\n",
      "###########################\n",
      "epoch: 230/300\n",
      "217.11780858039856\n",
      "train loss: 0.588134\n",
      "####\n",
      "###########################\n",
      "epoch: 231/300\n",
      "217.1674098968506\n",
      "train loss: 0.636839\n",
      "####\n",
      "###########################\n",
      "epoch: 232/300\n",
      "216.19360089302063\n",
      "train loss: 0.595487\n",
      "####\n",
      "###########################\n",
      "epoch: 233/300\n",
      "215.44275045394897\n",
      "train loss: 0.584636\n",
      "####\n",
      "###########################\n",
      "epoch: 234/300\n",
      "216.83697628974915\n",
      "train loss: 0.613464\n",
      "####\n",
      "###########################\n",
      "epoch: 235/300\n",
      "223.063973903656\n",
      "train loss: 0.57981\n",
      "####\n",
      "###########################\n",
      "epoch: 236/300\n",
      "220.7757923603058\n",
      "train loss: 0.604398\n",
      "####\n",
      "###########################\n",
      "epoch: 237/300\n",
      "216.53170680999756\n",
      "train loss: 0.584814\n",
      "####\n",
      "###########################\n",
      "epoch: 238/300\n",
      "216.77777361869812\n",
      "train loss: 0.597085\n",
      "####\n",
      "###########################\n",
      "epoch: 239/300\n",
      "216.79646110534668\n",
      "train loss: 0.592791\n",
      "####\n",
      "###########################\n",
      "epoch: 240/300\n",
      "216.49677777290344\n",
      "train loss: 0.604447\n",
      "####\n",
      "###########################\n",
      "epoch: 241/300\n",
      "218.60876607894897\n",
      "train loss: 0.620547\n",
      "####\n",
      "###########################\n",
      "epoch: 242/300\n",
      "218.67015957832336\n",
      "train loss: 0.599763\n",
      "####\n",
      "###########################\n",
      "epoch: 243/300\n",
      "219.6181743144989\n",
      "train loss: 0.60206\n",
      "####\n",
      "###########################\n",
      "epoch: 244/300\n",
      "216.86932253837585\n",
      "train loss: 0.599619\n",
      "####\n",
      "###########################\n",
      "epoch: 245/300\n",
      "217.49037790298462\n",
      "train loss: 0.585485\n",
      "####\n",
      "###########################\n",
      "epoch: 246/300\n",
      "217.22971034049988\n",
      "train loss: 0.605904\n",
      "####\n",
      "###########################\n",
      "epoch: 247/300\n",
      "217.06870555877686\n",
      "train loss: 0.592363\n",
      "####\n",
      "###########################\n",
      "epoch: 248/300\n",
      "216.4871587753296\n",
      "train loss: 0.6082\n",
      "####\n",
      "###########################\n",
      "epoch: 249/300\n",
      "217.0548496246338\n",
      "train loss: 0.596817\n",
      "####\n",
      "###########################\n",
      "epoch: 250/300\n",
      "216.89874744415283\n",
      "train loss: 0.591321\n",
      "####\n",
      "###########################\n",
      "epoch: 251/300\n",
      "216.95050859451294\n",
      "train loss: 0.587976\n",
      "####\n",
      "###########################\n",
      "epoch: 252/300\n",
      "221.5195746421814\n",
      "train loss: 0.594841\n",
      "####\n",
      "###########################\n",
      "epoch: 253/300\n",
      "226.21674871444702\n",
      "train loss: 0.580782\n",
      "####\n",
      "###########################\n",
      "epoch: 254/300\n",
      "220.72920083999634\n",
      "train loss: 0.570443\n",
      "####\n",
      "###########################\n",
      "epoch: 255/300\n",
      "215.96645045280457\n",
      "train loss: 0.577708\n",
      "####\n",
      "###########################\n",
      "epoch: 256/300\n",
      "220.06951999664307\n",
      "train loss: 0.582371\n",
      "####\n",
      "###########################\n",
      "epoch: 257/300\n",
      "217.86276531219482\n",
      "train loss: 0.584437\n",
      "####\n",
      "###########################\n",
      "epoch: 258/300\n",
      "218.1703236103058\n",
      "train loss: 0.588868\n",
      "####\n",
      "###########################\n",
      "epoch: 259/300\n",
      "216.9190318584442\n",
      "train loss: 0.602329\n",
      "####\n",
      "###########################\n",
      "epoch: 260/300\n",
      "222.1080322265625\n",
      "train loss: 0.617011\n",
      "####\n",
      "###########################\n",
      "epoch: 261/300\n",
      "216.97136521339417\n",
      "train loss: 0.585606\n",
      "####\n",
      "###########################\n",
      "epoch: 262/300\n",
      "216.81542801856995\n",
      "train loss: 0.599275\n",
      "####\n",
      "###########################\n",
      "epoch: 263/300\n",
      "216.4370288848877\n",
      "train loss: 0.579188\n",
      "####\n",
      "###########################\n",
      "epoch: 264/300\n",
      "219.0723798274994\n",
      "train loss: 0.574249\n",
      "####\n",
      "###########################\n",
      "epoch: 265/300\n",
      "217.36506724357605\n",
      "train loss: 0.613234\n",
      "####\n",
      "###########################\n",
      "epoch: 266/300\n",
      "215.533607006073\n",
      "train loss: 0.588285\n",
      "####\n",
      "###########################\n",
      "epoch: 267/300\n",
      "217.4135193824768\n",
      "train loss: 0.589765\n",
      "####\n",
      "###########################\n",
      "epoch: 268/300\n",
      "220.5508053302765\n",
      "train loss: 0.585715\n",
      "####\n",
      "###########################\n",
      "epoch: 269/300\n",
      "216.75580978393555\n",
      "train loss: 0.567365\n",
      "####\n",
      "###########################\n",
      "epoch: 270/300\n",
      "216.81862711906433\n",
      "train loss: 0.582909\n",
      "####\n",
      "###########################\n",
      "epoch: 271/300\n",
      "216.64263081550598\n",
      "train loss: 0.586289\n",
      "####\n",
      "###########################\n",
      "epoch: 272/300\n",
      "217.26690196990967\n",
      "train loss: 0.593257\n",
      "####\n",
      "###########################\n",
      "epoch: 273/300\n",
      "216.65892267227173\n",
      "train loss: 0.567977\n",
      "####\n",
      "###########################\n",
      "epoch: 274/300\n",
      "217.53932285308838\n",
      "train loss: 0.593415\n",
      "####\n",
      "###########################\n",
      "epoch: 275/300\n",
      "217.69602608680725\n",
      "train loss: 0.561545\n",
      "####\n",
      "###########################\n",
      "epoch: 276/300\n",
      "216.9621226787567\n",
      "train loss: 0.575526\n",
      "####\n",
      "###########################\n",
      "epoch: 277/300\n",
      "216.48348689079285\n",
      "train loss: 0.568775\n",
      "####\n",
      "###########################\n",
      "epoch: 278/300\n",
      "217.47351956367493\n",
      "train loss: 0.574204\n",
      "####\n",
      "###########################\n",
      "epoch: 279/300\n",
      "220.1139898300171\n",
      "train loss: 0.593844\n",
      "####\n",
      "###########################\n",
      "epoch: 280/300\n",
      "216.4483950138092\n",
      "train loss: 0.575643\n",
      "####\n",
      "###########################\n",
      "epoch: 281/300\n",
      "215.55356645584106\n",
      "train loss: 0.583425\n",
      "####\n",
      "###########################\n",
      "epoch: 282/300\n",
      "215.08489871025085\n",
      "train loss: 0.589635\n",
      "####\n",
      "###########################\n",
      "epoch: 283/300\n",
      "213.9683861732483\n",
      "train loss: 0.589271\n",
      "####\n",
      "###########################\n",
      "epoch: 284/300\n",
      "233.704528093338\n",
      "train loss: 0.580749\n",
      "####\n",
      "###########################\n",
      "epoch: 285/300\n",
      "244.98742628097534\n",
      "train loss: 0.564157\n",
      "####\n",
      "###########################\n",
      "epoch: 286/300\n",
      "216.76439666748047\n",
      "train loss: 0.579814\n",
      "####\n",
      "###########################\n",
      "epoch: 287/300\n",
      "216.66065049171448\n",
      "train loss: 0.591916\n",
      "####\n",
      "###########################\n",
      "epoch: 288/300\n",
      "215.9176881313324\n",
      "train loss: 0.592941\n",
      "####\n",
      "###########################\n",
      "epoch: 289/300\n",
      "219.1173701286316\n",
      "train loss: 0.566586\n",
      "####\n",
      "###########################\n",
      "epoch: 290/300\n",
      "216.78719067573547\n",
      "train loss: 0.565388\n",
      "####\n",
      "###########################\n",
      "epoch: 291/300\n",
      "218.62737464904785\n",
      "train loss: 0.580029\n",
      "####\n",
      "###########################\n",
      "epoch: 292/300\n",
      "217.7442066669464\n",
      "train loss: 0.604855\n",
      "####\n",
      "###########################\n",
      "epoch: 293/300\n",
      "221.03741574287415\n",
      "train loss: 0.567413\n",
      "####\n",
      "###########################\n",
      "epoch: 294/300\n",
      "218.06473517417908\n",
      "train loss: 0.577929\n",
      "####\n",
      "###########################\n",
      "epoch: 295/300\n",
      "220.1342580318451\n",
      "train loss: 0.562617\n",
      "####\n",
      "###########################\n",
      "epoch: 296/300\n",
      "217.54201984405518\n",
      "train loss: 0.562747\n",
      "####\n",
      "###########################\n",
      "epoch: 297/300\n",
      "217.99242997169495\n",
      "train loss: 0.576556\n",
      "####\n",
      "###########################\n",
      "epoch: 298/300\n",
      "218.23566198349\n",
      "train loss: 0.589454\n",
      "####\n",
      "###########################\n",
      "epoch: 299/300\n",
      "216.95142149925232\n",
      "train loss: 0.582619\n",
      "####\n",
      "###########################\n",
      "epoch: 300/300\n",
      "219.41387724876404\n",
      "train loss: 0.553345\n",
      "####\n"
     ]
    }
   ],
   "source": [
    "epoch_losses_train = []\n",
    "epoch_losses_val = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print (\"###########################\")\n",
    "    print (\"epoch: %d/%d\" % (epoch+1, num_epochs))\n",
    "\n",
    "    ############################################################################\n",
    "    # train:\n",
    "    ############################################################################\n",
    "    network.train() # (set in training mode, this affects BatchNorm and dropout)\n",
    "    batch_losses = []\n",
    "    current_time = time.time()\n",
    "    for step, (imgs, label_imgs) in enumerate(train_loader):\n",
    "        #current_time = time.time()\n",
    "\n",
    "        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
    "        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
    "\n",
    "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
    "\n",
    "        # compute the loss:\n",
    "        loss = loss_fn(outputs, label_imgs)\n",
    "        loss_value = loss.data.cpu().numpy()\n",
    "        batch_losses.append(loss_value)\n",
    "\n",
    "        # optimization step:\n",
    "        optimizer.zero_grad() # (reset gradients)\n",
    "        loss.backward()       # (compute gradients)\n",
    "        optimizer.step()      # (perform optimization step)\n",
    "\n",
    "    print (time.time() - current_time)\n",
    "\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    epoch_losses_train.append(epoch_loss)\n",
    "    with open(\"%s/epoch_losses_train.pkl\" % network.model_dir, \"wb\") as file:\n",
    "        pickle.dump(epoch_losses_train, file) \n",
    "        \n",
    "    print (\"train loss: %g\" % epoch_loss)\n",
    "    plt.figure(1)\n",
    "    plt.plot(epoch_losses_train, \"k^\")\n",
    "    plt.plot(epoch_losses_train, \"k\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"train loss per epoch\")\n",
    "    plt.savefig(\"%s/epoch_losses_train.png\" % network.model_dir)\n",
    "    plt.close(1)\n",
    "\n",
    "    print (\"####\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffec16-e883-457d-ab82-37cbbdd58dbf",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9951ff3-0315-4bf7-a582-205b779f04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.580649\n"
     ]
    }
   ],
   "source": [
    "    ############################################################################\n",
    "    # val:\n",
    "    ############################################################################\n",
    "    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
    "    batch_losses = []\n",
    "    for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
    "        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
    "            imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
    "            label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
    "\n",
    "            outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
    "\n",
    "            # compute the loss:\n",
    "            loss = loss_fn(outputs, label_imgs)\n",
    "            loss_value = loss.data.cpu().numpy()\n",
    "            batch_losses.append(loss_value)\n",
    "\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    epoch_losses_val.append(epoch_loss)\n",
    "    with open(\"%s/epoch_losses_val.pkl\" % network.model_dir, \"wb\") as file:\n",
    "        pickle.dump(epoch_losses_val, file)\n",
    "    print (\"val loss: %g\" % epoch_loss)\n",
    "    plt.figure(1)\n",
    "    plt.plot(epoch_losses_val, \"k^\")\n",
    "    plt.plot(epoch_losses_val, \"k\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"val loss per epoch\")\n",
    "    plt.savefig(\"%s/epoch_losses_val.png\" % network.model_dir)\n",
    "    plt.close(1)\n",
    "\n",
    "    # save the model weights to disk:\n",
    "    checkpoint_path = network.checkpoints_dir + \"/model_\" + model_id +\"_epoch_\" + str(epoch+1) + \".pth\"\n",
    "    torch.save(network.state_dict(), checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467be362-87c7-4170-abf6-c3315ffcc158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
